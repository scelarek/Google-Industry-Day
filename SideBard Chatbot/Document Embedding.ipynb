{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Embedding Any articles for search](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Embedding Any articles for search](#toc1_)    \n",
    "  - [Prerequisites](#toc1_1_)    \n",
    "    - [Import libraries](#toc1_1_1_)    \n",
    "    - [Set API key (if needed)](#toc1_1_2_)    \n",
    "  - [Collect documents](#toc1_2_)    \n",
    "  - [Chunk documents](#toc1_3_)    \n",
    "  - [Embed document chunks](#toc1_4_)    \n",
    "  - [Store document chunks and embeddings](#toc1_5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook shows how we prepared a dataset of Google Help articles for search, used in [Question_answering_using_embeddings.ipynb](Question_answering_using_embeddings.ipynb).\n",
    "\n",
    "Procedure:\n",
    "\n",
    "0. Prerequisites: Import libraries, set API key (if needed)\n",
    "1. Collect: We download a few hundred articles\n",
    "2. Chunk: Documents are split into short, semi-self-contained sections to be embedded\n",
    "3. Embed: Each section is embedded with the OpenAI API\n",
    "4. Store: Embeddings are saved in a CSV file (for large datasets, use a vector database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Prerequisites](#toc0_)\n",
    "\n",
    "### <a id='toc1_1_1_'></a>[Import libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai  # for generating embeddings\n",
    "import pandas as pd  # for DataFrames to store article sections and embeddings\n",
    "import re  # for cutting <ref> links out of Wikipedia articles\n",
    "import tiktoken  # for counting tokens\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install any missing libraries with `pip install` in your terminal. E.g.,\n",
    "\n",
    "```zsh\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "(You can also do this in a notebook cell with `!pip install openai`.)\n",
    "\n",
    "If you install any libraries, be sure to restart the notebook kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_2_'></a>[Set API key (if needed)](#toc0_)\n",
    "\n",
    "Note that the OpenAI library will try to read your API key from the `OPENAI_API_KEY` environment variable. If you haven't already, set this environment variable by following [these instructions](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Collect documents](#toc0_)\n",
    "\n",
    "In this example, we'll download a few hundred Wikipedia articles related to the 2022 Winter Olympics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_doc import *\n",
    "\n",
    "# API configuration\n",
    "openai.api_key = OPEN_AI_KEY\n",
    "\n",
    "# for LangChain\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPEN_AI_KEY\n",
    "\n",
    "def concat_rows(df):\n",
    "    flattened_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        flattened_row = ', '.join([f'{col}: {row[col]}' for col in df.columns])\n",
    "        flattened_data.append(flattened_row)\n",
    "    return flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the flattened data to a new dataframe\n",
    "\n",
    "df1 = pd.read_csv('Google Hackathon - Doc Sheet.csv')\n",
    "flat_df1 = concat_rows(df1)\n",
    "flat_df1 = pd.DataFrame(flat_df1, columns=['Concatenated Row'])\n",
    "\n",
    "\n",
    "df2 = pd.read_csv('Google Hackathon - Google Suite.csv', header=None)\n",
    "df2.rename(columns={0: 'Title', 1: 'Description'}, inplace=True)\n",
    "flat_df2 = concat_rows(df2)\n",
    "flat_df2 = pd.DataFrame(flat_df2, columns=['Concatenated Row'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Chunk documents](#toc0_)\n",
    "\n",
    "Now that we have our reference documents, we need to prepare them for search.\n",
    "\n",
    "Because GPT can only read a limited amount of text at once, we'll split each document into chunks short enough to be read.\n",
    "\n",
    "For this specific example on Wikipedia articles, we'll:\n",
    "- Discard less relevant-looking sections like External Links and Footnotes\n",
    "- Clean up the text by removing reference tags (e.g., <ref>), whitespace, and super short sections\n",
    "- Split each article into sections\n",
    "- Prepend titles and subtitles to each section's text, to help GPT understand the context\n",
    "- If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along semantic boundaries like paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll recursively split long sections into smaller sections.\n",
    "\n",
    "There's no perfect recipe for splitting text into sections.\n",
    "\n",
    "Some tradeoffs include:\n",
    "- Longer sections may be better for questions that require more context\n",
    "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
    "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
    "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
    "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
    "\n",
    "Here, we'll use a simple approach and limit sections to 1,600 tokens each, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
    "MAX_TOKENS = 1600\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"\n",
    "    Compute the number of tokens in a given text string for a specific model.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The input text string.\n",
    "    - model (str): Model identifier to be used for tokenization. Default is GPT_MODEL.\n",
    "    \n",
    "    Returns:\n",
    "    - int: Number of tokens in the input text for the given model.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "    \"\"\"\n",
    "    Splits a string into two parts based on a delimiter while trying to balance the tokens on each side.\n",
    "    \n",
    "    Parameters:\n",
    "    - string (str): Input text string.\n",
    "    - delimiter (str): Delimiter on which to split the string. Default is a newline character.\n",
    "    \n",
    "    Returns:\n",
    "    - list[str, str]: A list of two strings, representing the split halves.\n",
    "    \"\"\"\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]  # no delimiter found\n",
    "    elif len(chunks) == 2:\n",
    "        return chunks  # no need to search for halfway point\n",
    "    else:\n",
    "        total_tokens = num_tokens(string)\n",
    "        halfway = total_tokens // 2\n",
    "        best_diff = halfway\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            left = delimiter.join(chunks[: i + 1])\n",
    "            left_tokens = num_tokens(left)\n",
    "            diff = abs(halfway - left_tokens)\n",
    "            if diff >= best_diff:\n",
    "                break\n",
    "            else:\n",
    "                best_diff = diff\n",
    "        left = delimiter.join(chunks[:i])\n",
    "        right = delimiter.join(chunks[i:])\n",
    "        return [left, right]\n",
    "\n",
    "\n",
    "def truncated_string(\n",
    "    string: str,\n",
    "    model: str= GPT_MODEL,\n",
    "    max_tokens: int= MAX_TOKENS,\n",
    "    print_warning: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Truncate a string to fit within a maximum number of tokens.\n",
    "    \n",
    "    Parameters:\n",
    "    - string (str): Input text string.\n",
    "    - model (str): Model identifier to be used for truncation.\n",
    "    - max_tokens (int): Maximum number of tokens allowed.\n",
    "    - print_warning (bool): Whether to print a warning if truncation occurs. Default is True.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The truncated string.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string\n",
    "\n",
    "\n",
    "# sample = truncated_string(full_flattened_array[0])\n",
    "\n",
    "# sample_again = halved_by_delimiter(sample)\n",
    "\n",
    "# np.array(sample_again).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Embed document chunks](#toc0_)\n",
    "\n",
    "Now that we've split our library into shorter self-contained strings, we can compute embeddings for each.\n",
    "\n",
    "(For large embedding jobs, use a script like [api_request_parallel_processor.py](api_request_parallel_processor.py) to parallelize requests while throttling to stay under rate limits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_embeddings(full_flattened_array: list, EMBEDDING_MODEL: str = \"text-embedding-ada-002\", BATCH_SIZE: int = 1000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes embeddings for a given list of texts.\n",
    "    \n",
    "    Parameters:\n",
    "    - full_flattened_array (list): List of texts to compute embeddings for.\n",
    "    - EMBEDDING_MODEL (str): OpenAI model for embeddings. Default is \"text-embedding-ada-002\".\n",
    "    - BATCH_SIZE (int): Number of texts to be processed in a batch. Default is 1000.\n",
    "    \n",
    "    Returns:\n",
    "    - df (pd.DataFrame): DataFrame with original texts and their corresponding embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    embeddings = []\n",
    "\n",
    "    for batch_start in range(0, len(full_flattened_array), BATCH_SIZE):\n",
    "        batch_end = batch_start + BATCH_SIZE\n",
    "        batch = full_flattened_array[batch_start:batch_end]\n",
    "\n",
    "        # Attempt to create embeddings, with retries on rate limit errors\n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "                response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
    "                for i, be in enumerate(response[\"data\"]):\n",
    "                    assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
    "                batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "                embeddings.extend(batch_embeddings)\n",
    "                break  # Break out of the retry loop if successful\n",
    "            except openai.error.RateLimitError:\n",
    "                if attempt < max_retries - 1:  # i.e. not the last attempt\n",
    "                    print(\"Rate limit reached. Waiting for 60 seconds before retrying...\")\n",
    "                    time.sleep(60)  # Wait for 60 seconds before the next attempt\n",
    "                else:\n",
    "                    raise  # If it's the last attempt, raise the exception to alert the user\n",
    "\n",
    "    df = pd.DataFrame({\"text\": full_flattened_array, \"embedding\": embeddings})\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(531, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "full_flattened_set = pd.concat([flat_df2, flat_df1])\n",
    "\n",
    "full_flattened_array = np.array(full_flattened_set).squeeze().tolist()\n",
    "\n",
    "full_flattened_set.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# calculate embeddings\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
    "BATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "\n",
    "result_df = compute_embeddings(full_flattened_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Store document chunks and embeddings](#toc0_)\n",
    "\n",
    "Because this example only uses a few thousand strings, we'll store them in a CSV file.\n",
    "\n",
    "(For larger datasets, use a vector database, which will be more performant.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save document chunks and embeddings\n",
    "\n",
    "SAVE_PATH = \"google_docs.csv\"\n",
    "\n",
    "result_df.to_csv(SAVE_PATH, index=False)\n",
    "\n",
    "# d:\\Samsickle\\Downloads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
